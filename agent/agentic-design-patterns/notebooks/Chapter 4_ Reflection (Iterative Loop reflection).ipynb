{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Reflection (代码示例)\n",
    "\n",
    "本 notebook 演示了如何使用 LangChain 实现 Reflection（反思）模式，通过迭代循环不断改进代码。\n",
    "\n",
    "## 前置要求\n",
    "\n",
    "- Python 3.8+\n",
    "- 阿里云 DashScope API Key（需要设置环境变量 `DASHSCOPE_API_KEY`）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 安装依赖\n",
    "\n",
    "首先安装所需的 Python 包。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 安装必要的依赖包（使用 %pip 在 Jupyter 中更推荐）\n",
    "%pip install -q langchain-core>=0.1.0 langchain-community>=0.1.0 dashscope>=1.17.0 python-dotenv>=1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 获取 API Key\n",
    "\n",
    "如果环境变量或 .env 文件中没有设置 `DASHSCOPE_API_KEY`，可以在此处直接设置。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# 获取 API key（优先从环境变量读取，如果没有则从 .env 文件）\n",
    "dashscope_api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "if not dashscope_api_key:\n",
    "    print(\"警告：未找到 DASHSCOPE_API_KEY，请确保已设置环境变量或 .env 文件\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 导入库并初始化模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3NBB8hsfc7iJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model initialized: qwen-plus\n",
      "\n",
      "========================= REFLECTION LOOP: ITERATION 1 =========================\n",
      "\n",
      ">>> STAGE 1: GENERATING initial code...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generated Code (v1) ---\n",
      "```python\n",
      "def calculate_factorial(n):\n",
      "    \"\"\"\n",
      "    Calculate the factorial of a non-negative integer.\n",
      "    \n",
      "    The factorial of a number n (denoted as n!) is the product of all positive\n",
      "    integers less than or equal to n. By definition, 0! = 1.\n",
      "    \n",
      "    Args:\n",
      "        n (int): A non-negative integer for which to calculate the factorial.\n",
      "        \n",
      "    Returns:\n",
      "        int: The factorial of n (n!).\n",
      "        \n",
      "    Raises:\n",
      "        ValueError: If n is negative.\n",
      "        TypeError: If n is not an integer.\n",
      "        \n",
      "    Examples:\n",
      "        >>> calculate_factorial(5)\n",
      "        120\n",
      "        >>> calculate_factorial(0)\n",
      "        1\n",
      "        >>> calculate_factorial(3)\n",
      "        6\n",
      "    \"\"\"\n",
      "    # Check if input is an integer\n",
      "    if not isinstance(n, int):\n",
      "        raise TypeError(\"Input must be an integer\")\n",
      "    \n",
      "    # Handle negative numbers\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Factorial is not defined for negative numbers\")\n",
      "    \n",
      "    # Base case: 0! = 1\n",
      "    if n == 0 or n == 1:\n",
      "        return 1\n",
      "    \n",
      "    # Calculate factorial iteratively\n",
      "    result = 1\n",
      "    for i in range(2, n + 1):\n",
      "        result *= i\n",
      "    \n",
      "    return result\n",
      "```\n",
      "\n",
      ">>> STAGE 2: REFLECTING on the generated code...\n",
      "\n",
      "--- Critique ---\n",
      "No further critiques found. The code is satisfactory.\n",
      "\n",
      "============================== FINAL RESULT ==============================\n",
      "\n",
      "Final refined code after the reflection process:\n",
      "\n",
      "```python\n",
      "def calculate_factorial(n):\n",
      "    \"\"\"\n",
      "    Calculate the factorial of a non-negative integer.\n",
      "    \n",
      "    The factorial of a number n (denoted as n!) is the product of all positive\n",
      "    integers less than or equal to n. By definition, 0! = 1.\n",
      "    \n",
      "    Args:\n",
      "        n (int): A non-negative integer for which to calculate the factorial.\n",
      "        \n",
      "    Returns:\n",
      "        int: The factorial of n (n!).\n",
      "        \n",
      "    Raises:\n",
      "        ValueError: If n is negative.\n",
      "        TypeError: If n is not an integer.\n",
      "        \n",
      "    Examples:\n",
      "        >>> calculate_factorial(5)\n",
      "        120\n",
      "        >>> calculate_factorial(0)\n",
      "        1\n",
      "        >>> calculate_factorial(3)\n",
      "        6\n",
      "    \"\"\"\n",
      "    # Check if input is an integer\n",
      "    if not isinstance(n, int):\n",
      "        raise TypeError(\"Input must be an integer\")\n",
      "    \n",
      "    # Handle negative numbers\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Factorial is not defined for negative numbers\")\n",
      "    \n",
      "    # Base case: 0! = 1\n",
      "    if n == 0 or n == 1:\n",
      "        return 1\n",
      "    \n",
      "    # Calculate factorial iteratively\n",
      "    result = 1\n",
      "    for i in range(2, n + 1):\n",
      "        result *= i\n",
      "    \n",
      "    return result\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatTongyi\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# --- Configuration ---\n",
    "# 初始化 Qwen 语言模型（使用阿里云 DashScope API）\n",
    "# model 参数可以指定不同的 Qwen 模型，如 \"qwen-flash\", \"qwen-turbo\", \"qwen-plus\", \"qwen-max\" 等\n",
    "# 对于代码生成和反思任务，建议使用 qwen-plus 或 qwen-max 以获得更好的推理能力\n",
    "try:\n",
    "    llm = ChatTongyi(\n",
    "        model=\"qwen-plus\",  # 使用 qwen-plus 以获得更好的代码生成和反思能力\n",
    "        temperature=0.1,  # 较低的温度用于更确定性和聚焦的输出\n",
    "        dashscope_api_key=dashscope_api_key\n",
    "    )\n",
    "    print(f\"Language model initialized: qwen-plus\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing language model: {e}\")\n",
    "    llm = None\n",
    "\n",
    "\n",
    "def run_reflection_loop():\n",
    "    \"\"\"\n",
    "    Demonstrates a multi-step AI reflection loop to progressively improve a Python function.\n",
    "    \"\"\"\n",
    "    if not llm:\n",
    "        print(\"LLM 未初始化，无法运行反思循环。请检查 API key 配置。\")\n",
    "        return\n",
    "\n",
    "    # --- The Core Task ---\n",
    "    task_prompt = \"\"\"\n",
    "    Your task is to create a Python function named `calculate_factorial`.\n",
    "    This function should do the following:\n",
    "    1.  Accept a single integer `n` as input.\n",
    "    2.  Calculate its factorial (n!).\n",
    "    3.  Include a clear docstring explaining what the function does.\n",
    "    4.  Handle edge cases: The factorial of 0 is 1.\n",
    "    5.  Handle invalid input: Raise a ValueError if the input is a negative number.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- The Reflection Loop ---\n",
    "    max_iterations = 3\n",
    "    current_code = \"\"\n",
    "    # We will build a conversation history to provide context in each step.\n",
    "    message_history = [HumanMessage(content=task_prompt)]\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        print(\"\\n\" + \"=\"*25 + f\" REFLECTION LOOP: ITERATION {i + 1} \" + \"=\"*25)\n",
    "\n",
    "        # --- 1. GENERATE / REFINE STAGE ---\n",
    "        # In the first iteration, it generates. In subsequent iterations, it refines.\n",
    "        if i == 0:\n",
    "            print(\"\\n>>> STAGE 1: GENERATING initial code...\")\n",
    "            # The first message is just the task prompt.\n",
    "            response = llm.invoke(message_history)\n",
    "            current_code = response.content\n",
    "        else:\n",
    "            print(\"\\n>>> STAGE 1: REFINING code based on previous critique...\")\n",
    "            # The message history now contains the task, the last code, and the last critique.\n",
    "            # We instruct the model to apply the critiques.\n",
    "            message_history.append(HumanMessage(content=\"Please refine the code using the critiques provided.\"))\n",
    "            response = llm.invoke(message_history)\n",
    "            current_code = response.content\n",
    "\n",
    "        print(\"\\n--- Generated Code (v\" + str(i + 1) + \") ---\\n\" + current_code)\n",
    "        message_history.append(response) # Add the generated code to history\n",
    "\n",
    "        # --- 2. REFLECT STAGE ---\n",
    "        print(\"\\n>>> STAGE 2: REFLECTING on the generated code...\")\n",
    "\n",
    "        # Create a specific prompt for the reflector agent.\n",
    "        # This asks the model to act as a senior code reviewer.\n",
    "        reflector_prompt = [\n",
    "            SystemMessage(content=\"\"\"\n",
    "                You are a senior software engineer and an expert in Python.\n",
    "                Your role is to perform a meticulous code review.\n",
    "                Critically evaluate the provided Python code based on the original task requirements.\n",
    "                Look for bugs, style issues, missing edge cases, and areas for improvement.\n",
    "                If the code is perfect and meets all requirements, respond with the single phrase 'CODE_IS_PERFECT'.\n",
    "                Otherwise, provide a bulleted list of your critiques.\n",
    "            \"\"\"),\n",
    "            HumanMessage(content=f\"Original Task:\\n{task_prompt}\\n\\nCode to Review:\\n{current_code}\")\n",
    "        ]\n",
    "\n",
    "        critique_response = llm.invoke(reflector_prompt)\n",
    "        critique = critique_response.content\n",
    "\n",
    "        # --- 3. STOPPING CONDITION ---\n",
    "        if \"CODE_IS_PERFECT\" in critique:\n",
    "            print(\"\\n--- Critique ---\\nNo further critiques found. The code is satisfactory.\")\n",
    "            break\n",
    "\n",
    "        print(\"\\n--- Critique ---\\n\" + critique)\n",
    "        # Add the critique to the history for the next refinement loop.\n",
    "        message_history.append(HumanMessage(content=f\"Critique of the previous code:\\n{critique}\"))\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30 + \" FINAL RESULT \" + \"=\"*30)\n",
    "    print(\"\\nFinal refined code after the reflection process:\\n\")\n",
    "    print(current_code)\n",
    "\n",
    "\n",
    "# 运行反思循环示例\n",
    "# 在 Jupyter notebook 中，可以直接运行此函数\n",
    "if llm:\n",
    "    run_reflection_loop()\n",
    "else:\n",
    "    print(\"LLM 未初始化，无法运行示例。请检查 API key 配置。\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1vt3Wx0n-4iJnkREJ3UFhwVYkh2yagpzh",
     "timestamp": 1750141768092
    }
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
