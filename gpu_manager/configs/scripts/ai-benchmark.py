#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIåŸºå‡†æµ‹è¯•è„šæœ¬
æä¾›å…¨é¢çš„AIæ¨¡å‹æ€§èƒ½æµ‹è¯•å’ŒåŸºå‡†æµ‹è¯•åŠŸèƒ½
æ”¯æŒPyTorchã€TensorFlowã€ONNXç­‰æ¡†æ¶
"""

import os
import sys
import time
import json
import argparse
import subprocess
import numpy as np
from datetime import datetime
from typing import Dict, List, Tuple, Optional

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import torchvision.models as models
    import torchvision.transforms as transforms
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("è­¦å‘Š: PyTorchæœªå®‰è£…ï¼Œå°†è·³è¿‡PyTorchç›¸å…³æµ‹è¯•")

try:
    import tensorflow as tf
    TF_AVAILABLE = True
except ImportError:
    TF_AVAILABLE = False
    print("è­¦å‘Š: TensorFlowæœªå®‰è£…ï¼Œå°†è·³è¿‡TensorFlowç›¸å…³æµ‹è¯•")

try:
    import onnxruntime as ort
    ONNX_AVAILABLE = True
except ImportError:
    ONNX_AVAILABLE = False
    print("è­¦å‘Š: ONNX Runtimeæœªå®‰è£…ï¼Œå°†è·³è¿‡ONNXç›¸å…³æµ‹è¯•")

class AIBenchmark:
    """AIåŸºå‡†æµ‹è¯•ç±»"""
    
    def __init__(self, output_dir: str = None, device: str = 'auto'):
        self.output_dir = output_dir or f"ai_benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.device = self._setup_device(device)
        self.results = {}
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—æ–‡ä»¶
        self.log_file = os.path.join(self.output_dir, 'benchmark.log')
        
        print(f"AIåŸºå‡†æµ‹è¯•åˆå§‹åŒ–å®Œæˆ")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"è®¡ç®—è®¾å¤‡: {self.device}")
    
    def _setup_device(self, device: str) -> str:
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if device == 'auto':
            if TORCH_AVAILABLE and torch.cuda.is_available():
                return 'cuda'
            elif TF_AVAILABLE and tf.config.list_physical_devices('GPU'):
                return 'gpu'
            else:
                return 'cpu'
        return device
    
    def log(self, message: str, level: str = 'INFO'):
        """è®°å½•æ—¥å¿—"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_message = f"[{timestamp}] [{level}] {message}"
        print(log_message)
        
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(log_message + '\n')
    
    def get_system_info(self) -> Dict:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        info = {
            'timestamp': datetime.now().isoformat(),
            'python_version': sys.version,
            'platform': sys.platform
        }
        
        # GPUä¿¡æ¯
        try:
            result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,compute_cap', 
                                   '--format=csv,noheader'], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                gpu_info = []
                for line in result.stdout.strip().split('\n'):
                    if line.strip():
                        parts = line.split(', ')
                        if len(parts) >= 3:
                            gpu_info.append({
                                'name': parts[0],
                                'memory': parts[1],
                                'compute_capability': parts[2]
                            })
                info['gpus'] = gpu_info
        except Exception as e:
            info['gpu_error'] = str(e)
        
        # æ¡†æ¶ç‰ˆæœ¬
        if TORCH_AVAILABLE:
            info['pytorch_version'] = torch.__version__
            info['cuda_available'] = torch.cuda.is_available()
            if torch.cuda.is_available():
                info['cuda_version'] = torch.version.cuda
                info['cudnn_version'] = torch.backends.cudnn.version()
        
        if TF_AVAILABLE:
            info['tensorflow_version'] = tf.__version__
            info['tf_gpu_available'] = len(tf.config.list_physical_devices('GPU')) > 0
        
        if ONNX_AVAILABLE:
            info['onnxruntime_version'] = ort.__version__
        
        return info
    
    def pytorch_benchmark(self) -> Dict:
        """PyTorchåŸºå‡†æµ‹è¯•"""
        if not TORCH_AVAILABLE:
            return {'error': 'PyTorch not available'}
        
        self.log("å¼€å§‹PyTorchåŸºå‡†æµ‹è¯•")
        results = {}
        
        device = torch.device(self.device if self.device != 'gpu' else 'cuda')
        
        # 1. å›¾åƒåˆ†ç±»æ¨¡å‹æµ‹è¯•
        self.log("æµ‹è¯•å›¾åƒåˆ†ç±»æ¨¡å‹...")
        classification_results = self._test_pytorch_classification(device)
        results['classification'] = classification_results
        
        # 2. è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹æµ‹è¯•
        self.log("æµ‹è¯•NLPæ¨¡å‹...")
        nlp_results = self._test_pytorch_nlp(device)
        results['nlp'] = nlp_results
        
        # 3. å†…å­˜å’Œè®¡ç®—æ€§èƒ½æµ‹è¯•
        self.log("æµ‹è¯•å†…å­˜å’Œè®¡ç®—æ€§èƒ½...")
        compute_results = self._test_pytorch_compute(device)
        results['compute'] = compute_results
        
        return results
    
    def _test_pytorch_classification(self, device) -> Dict:
        """æµ‹è¯•PyTorchå›¾åƒåˆ†ç±»æ¨¡å‹"""
        results = {}
        
        # æµ‹è¯•ä¸åŒæ¨¡å‹
        models_to_test = [
            ('ResNet18', models.resnet18),
            ('ResNet50', models.resnet50),
            ('VGG16', models.vgg16),
            ('MobileNetV2', models.mobilenet_v2)
        ]
        
        batch_sizes = [1, 4, 8, 16, 32]
        input_size = (3, 224, 224)
        
        for model_name, model_fn in models_to_test:
            self.log(f"æµ‹è¯•æ¨¡å‹: {model_name}")
            model_results = {}
            
            try:
                model = model_fn(pretrained=False).to(device)
                model.eval()
                
                for batch_size in batch_sizes:
                    # åˆ›å»ºéšæœºè¾“å…¥
                    input_tensor = torch.randn(batch_size, *input_size).to(device)
                    
                    # é¢„çƒ­
                    with torch.no_grad():
                        for _ in range(10):
                            _ = model(input_tensor)
                    
                    if device.type == 'cuda':
                        torch.cuda.synchronize()
                    
                    # æ¨ç†æ€§èƒ½æµ‹è¯•
                    start_time = time.time()
                    with torch.no_grad():
                        for _ in range(100):
                            output = model(input_tensor)
                    
                    if device.type == 'cuda':
                        torch.cuda.synchronize()
                    
                    end_time = time.time()
                    
                    avg_time = (end_time - start_time) / 100
                    throughput = batch_size / avg_time
                    
                    model_results[f'batch_{batch_size}'] = {
                        'avg_time_ms': avg_time * 1000,
                        'throughput_samples_per_sec': throughput
                    }
                
                results[model_name] = model_results
                
            except Exception as e:
                self.log(f"æ¨¡å‹ {model_name} æµ‹è¯•å¤±è´¥: {str(e)}", 'ERROR')
                results[model_name] = {'error': str(e)}
        
        return results
    
    def _test_pytorch_nlp(self, device) -> Dict:
        """æµ‹è¯•PyTorch NLPæ¨¡å‹"""
        results = {}
        
        # Transformeræ¨¡å‹æµ‹è¯•
        try:
            # ç®€å•çš„Transformerç¼–ç å™¨
            class SimpleTransformer(nn.Module):
                def __init__(self, vocab_size=10000, d_model=512, nhead=8, num_layers=6):
                    super().__init__()
                    self.embedding = nn.Embedding(vocab_size, d_model)
                    self.pos_encoding = nn.Parameter(torch.randn(1000, d_model))
                    encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, batch_first=True)
                    self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
                    self.fc = nn.Linear(d_model, vocab_size)
                
                def forward(self, x):
                    seq_len = x.size(1)
                    x = self.embedding(x) + self.pos_encoding[:seq_len]
                    x = self.transformer(x)
                    return self.fc(x)
            
            model = SimpleTransformer().to(device)
            model.eval()
            
            sequence_lengths = [128, 256, 512]
            batch_sizes = [1, 4, 8, 16]
            
            for seq_len in sequence_lengths:
                for batch_size in batch_sizes:
                    # åˆ›å»ºéšæœºè¾“å…¥
                    input_ids = torch.randint(0, 10000, (batch_size, seq_len)).to(device)
                    
                    # é¢„çƒ­
                    with torch.no_grad():
                        for _ in range(5):
                            _ = model(input_ids)
                    
                    if device.type == 'cuda':
                        torch.cuda.synchronize()
                    
                    # æ€§èƒ½æµ‹è¯•
                    start_time = time.time()
                    with torch.no_grad():
                        for _ in range(20):
                            output = model(input_ids)
                    
                    if device.type == 'cuda':
                        torch.cuda.synchronize()
                    
                    end_time = time.time()
                    
                    avg_time = (end_time - start_time) / 20
                    throughput = batch_size / avg_time
                    
                    key = f'seq_{seq_len}_batch_{batch_size}'
                    results[key] = {
                        'avg_time_ms': avg_time * 1000,
                        'throughput_samples_per_sec': throughput
                    }
        
        except Exception as e:
            self.log(f"NLPæ¨¡å‹æµ‹è¯•å¤±è´¥: {str(e)}", 'ERROR')
            results['error'] = str(e)
        
        return results
    
    def _test_pytorch_compute(self, device) -> Dict:
        """æµ‹è¯•PyTorchè®¡ç®—æ€§èƒ½"""
        results = {}
        
        # çŸ©é˜µä¹˜æ³•æµ‹è¯•
        matrix_sizes = [512, 1024, 2048, 4096]
        
        for size in matrix_sizes:
            try:
                a = torch.randn(size, size).to(device)
                b = torch.randn(size, size).to(device)
                
                # é¢„çƒ­
                for _ in range(10):
                    _ = torch.mm(a, b)
                
                if device.type == 'cuda':
                    torch.cuda.synchronize()
                
                # æ€§èƒ½æµ‹è¯•
                start_time = time.time()
                for _ in range(100):
                    c = torch.mm(a, b)
                
                if device.type == 'cuda':
                    torch.cuda.synchronize()
                
                end_time = time.time()
                
                avg_time = (end_time - start_time) / 100
                flops = 2 * size ** 3
                gflops = flops / avg_time / 1e9
                
                results[f'matmul_{size}x{size}'] = {
                    'avg_time_ms': avg_time * 1000,
                    'gflops': gflops
                }
                
            except Exception as e:
                self.log(f"çŸ©é˜µä¹˜æ³•æµ‹è¯•å¤±è´¥ (size={size}): {str(e)}", 'ERROR')
        
        return results
    
    def tensorflow_benchmark(self) -> Dict:
        """TensorFlowåŸºå‡†æµ‹è¯•"""
        if not TF_AVAILABLE:
            return {'error': 'TensorFlow not available'}
        
        self.log("å¼€å§‹TensorFlowåŸºå‡†æµ‹è¯•")
        results = {}
        
        # è®¾ç½®GPUå†…å­˜å¢é•¿
        if self.device == 'gpu':
            gpus = tf.config.experimental.list_physical_devices('GPU')
            if gpus:
                try:
                    for gpu in gpus:
                        tf.config.experimental.set_memory_growth(gpu, True)
                except RuntimeError as e:
                    self.log(f"GPUé…ç½®é”™è¯¯: {e}", 'ERROR')
        
        # 1. å›¾åƒåˆ†ç±»æ¨¡å‹æµ‹è¯•
        self.log("æµ‹è¯•TensorFlowå›¾åƒåˆ†ç±»æ¨¡å‹...")
        classification_results = self._test_tf_classification()
        results['classification'] = classification_results
        
        # 2. è®¡ç®—æ€§èƒ½æµ‹è¯•
        self.log("æµ‹è¯•TensorFlowè®¡ç®—æ€§èƒ½...")
        compute_results = self._test_tf_compute()
        results['compute'] = compute_results
        
        return results
    
    def _test_tf_classification(self) -> Dict:
        """æµ‹è¯•TensorFlowå›¾åƒåˆ†ç±»æ¨¡å‹"""
        results = {}
        
        # æµ‹è¯•ä¸åŒæ¨¡å‹
        models_to_test = [
            ('MobileNetV2', tf.keras.applications.MobileNetV2),
            ('ResNet50', tf.keras.applications.ResNet50),
            ('VGG16', tf.keras.applications.VGG16)
        ]
        
        batch_sizes = [1, 4, 8, 16]
        input_shape = (224, 224, 3)
        
        for model_name, model_fn in models_to_test:
            self.log(f"æµ‹è¯•TensorFlowæ¨¡å‹: {model_name}")
            model_results = {}
            
            try:
                model = model_fn(weights=None, input_shape=input_shape)
                
                for batch_size in batch_sizes:
                    # åˆ›å»ºéšæœºè¾“å…¥
                    input_data = tf.random.normal((batch_size,) + input_shape)
                    
                    # é¢„çƒ­
                    for _ in range(10):
                        _ = model(input_data, training=False)
                    
                    # æ€§èƒ½æµ‹è¯•
                    start_time = time.time()
                    for _ in range(50):
                        output = model(input_data, training=False)
                    
                    end_time = time.time()
                    
                    avg_time = (end_time - start_time) / 50
                    throughput = batch_size / avg_time
                    
                    model_results[f'batch_{batch_size}'] = {
                        'avg_time_ms': avg_time * 1000,
                        'throughput_samples_per_sec': throughput
                    }
                
                results[model_name] = model_results
                
            except Exception as e:
                self.log(f"TensorFlowæ¨¡å‹ {model_name} æµ‹è¯•å¤±è´¥: {str(e)}", 'ERROR')
                results[model_name] = {'error': str(e)}
        
        return results
    
    def _test_tf_compute(self) -> Dict:
        """æµ‹è¯•TensorFlowè®¡ç®—æ€§èƒ½"""
        results = {}
        
        # çŸ©é˜µä¹˜æ³•æµ‹è¯•
        matrix_sizes = [512, 1024, 2048, 4096]
        
        for size in matrix_sizes:
            try:
                a = tf.random.normal((size, size))
                b = tf.random.normal((size, size))
                
                # é¢„çƒ­
                for _ in range(10):
                    _ = tf.matmul(a, b)
                
                # æ€§èƒ½æµ‹è¯•
                start_time = time.time()
                for _ in range(100):
                    c = tf.matmul(a, b)
                
                end_time = time.time()
                
                avg_time = (end_time - start_time) / 100
                flops = 2 * size ** 3
                gflops = flops / avg_time / 1e9
                
                results[f'matmul_{size}x{size}'] = {
                    'avg_time_ms': avg_time * 1000,
                    'gflops': gflops
                }
                
            except Exception as e:
                self.log(f"TensorFlowçŸ©é˜µä¹˜æ³•æµ‹è¯•å¤±è´¥ (size={size}): {str(e)}", 'ERROR')
        
        return results
    
    def onnx_benchmark(self) -> Dict:
        """ONNX RuntimeåŸºå‡†æµ‹è¯•"""
        if not ONNX_AVAILABLE:
            return {'error': 'ONNX Runtime not available'}
        
        self.log("å¼€å§‹ONNX RuntimeåŸºå‡†æµ‹è¯•")
        results = {}
        
        try:
            # åˆ›å»ºç®€å•çš„ONNXæ¨¡å‹è¿›è¡Œæµ‹è¯•
            if TORCH_AVAILABLE:
                results = self._test_onnx_with_pytorch()
            else:
                results['error'] = 'PyTorch required for ONNX model creation'
        
        except Exception as e:
            self.log(f"ONNXæµ‹è¯•å¤±è´¥: {str(e)}", 'ERROR')
            results['error'] = str(e)
        
        return results
    
    def _test_onnx_with_pytorch(self) -> Dict:
        """ä½¿ç”¨PyTorchåˆ›å»ºONNXæ¨¡å‹è¿›è¡Œæµ‹è¯•"""
        results = {}
        
        try:
            # åˆ›å»ºç®€å•çš„CNNæ¨¡å‹
            class SimpleCNN(nn.Module):
                def __init__(self):
                    super().__init__()
                    self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
                    self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
                    self.pool = nn.AdaptiveAvgPool2d((1, 1))
                    self.fc = nn.Linear(64, 10)
                
                def forward(self, x):
                    x = torch.relu(self.conv1(x))
                    x = torch.relu(self.conv2(x))
                    x = self.pool(x)
                    x = x.view(x.size(0), -1)
                    return self.fc(x)
            
            model = SimpleCNN()
            model.eval()
            
            # å¯¼å‡ºONNXæ¨¡å‹
            dummy_input = torch.randn(1, 3, 224, 224)
            onnx_path = os.path.join(self.output_dir, 'simple_cnn.onnx')
            
            torch.onnx.export(
                model, dummy_input, onnx_path,
                input_names=['input'], output_names=['output'],
                dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}
            )
            
            # åˆ›å»ºONNX Runtimeä¼šè¯
            providers = ['CPUExecutionProvider']
            if self.device == 'cuda' and 'CUDAExecutionProvider' in ort.get_available_providers():
                providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
            
            session = ort.InferenceSession(onnx_path, providers=providers)
            
            # æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°
            batch_sizes = [1, 4, 8, 16]
            
            for batch_size in batch_sizes:
                input_data = np.random.randn(batch_size, 3, 224, 224).astype(np.float32)
                
                # é¢„çƒ­
                for _ in range(10):
                    _ = session.run(['output'], {'input': input_data})
                
                # æ€§èƒ½æµ‹è¯•
                start_time = time.time()
                for _ in range(100):
                    output = session.run(['output'], {'input': input_data})
                
                end_time = time.time()
                
                avg_time = (end_time - start_time) / 100
                throughput = batch_size / avg_time
                
                results[f'batch_{batch_size}'] = {
                    'avg_time_ms': avg_time * 1000,
                    'throughput_samples_per_sec': throughput
                }
        
        except Exception as e:
            self.log(f"ONNXæ¨¡å‹æµ‹è¯•å¤±è´¥: {str(e)}", 'ERROR')
            results['error'] = str(e)
        
        return results
    
    def memory_benchmark(self) -> Dict:
        """å†…å­˜æ€§èƒ½æµ‹è¯•"""
        self.log("å¼€å§‹å†…å­˜æ€§èƒ½æµ‹è¯•")
        results = {}
        
        if TORCH_AVAILABLE and self.device == 'cuda':
            results['pytorch_cuda'] = self._test_pytorch_memory()
        
        if TF_AVAILABLE and self.device == 'gpu':
            results['tensorflow_gpu'] = self._test_tf_memory()
        
        return results
    
    def _test_pytorch_memory(self) -> Dict:
        """æµ‹è¯•PyTorch CUDAå†…å­˜æ€§èƒ½"""
        results = {}
        device = torch.device('cuda')
        
        # å†…å­˜åˆ†é…æµ‹è¯•
        sizes = [1024, 2048, 4096, 8192]
        
        for size in sizes:
            try:
                # æ¸…ç©ºç¼“å­˜
                torch.cuda.empty_cache()
                
                # è®°å½•åˆå§‹å†…å­˜
                initial_memory = torch.cuda.memory_allocated()
                
                # åˆ†é…å†…å­˜
                start_time = time.time()
                tensor = torch.randn(size, size, device=device)
                alloc_time = time.time() - start_time
                
                # è®°å½•åˆ†é…åå†…å­˜
                allocated_memory = torch.cuda.memory_allocated() - initial_memory
                
                # å†…å­˜å¤åˆ¶æµ‹è¯•
                start_time = time.time()
                tensor_copy = tensor.clone()
                copy_time = time.time() - start_time
                
                # é‡Šæ”¾å†…å­˜
                del tensor, tensor_copy
                torch.cuda.empty_cache()
                
                results[f'size_{size}x{size}'] = {
                    'allocation_time_ms': alloc_time * 1000,
                    'copy_time_ms': copy_time * 1000,
                    'memory_mb': allocated_memory / 1024 / 1024
                }
                
            except Exception as e:
                self.log(f"PyTorchå†…å­˜æµ‹è¯•å¤±è´¥ (size={size}): {str(e)}", 'ERROR')
        
        return results
    
    def _test_tf_memory(self) -> Dict:
        """æµ‹è¯•TensorFlow GPUå†…å­˜æ€§èƒ½"""
        results = {}
        
        # TensorFlowå†…å­˜æµ‹è¯•ç›¸å¯¹ç®€å•
        sizes = [1024, 2048, 4096]
        
        for size in sizes:
            try:
                # åˆ†é…å†…å­˜
                start_time = time.time()
                tensor = tf.random.normal((size, size))
                alloc_time = time.time() - start_time
                
                # å†…å­˜å¤åˆ¶æµ‹è¯•
                start_time = time.time()
                tensor_copy = tf.identity(tensor)
                copy_time = time.time() - start_time
                
                results[f'size_{size}x{size}'] = {
                    'allocation_time_ms': alloc_time * 1000,
                    'copy_time_ms': copy_time * 1000
                }
                
            except Exception as e:
                self.log(f"TensorFlowå†…å­˜æµ‹è¯•å¤±è´¥ (size={size}): {str(e)}", 'ERROR')
        
        return results
    
    def run_all_benchmarks(self) -> Dict:
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        self.log("å¼€å§‹AIåŸºå‡†æµ‹è¯•å¥—ä»¶")
        
        # è·å–ç³»ç»Ÿä¿¡æ¯
        system_info = self.get_system_info()
        self.results['system_info'] = system_info
        
        # è¿è¡Œå„é¡¹æµ‹è¯•
        if TORCH_AVAILABLE:
            self.results['pytorch'] = self.pytorch_benchmark()
        
        if TF_AVAILABLE:
            self.results['tensorflow'] = self.tensorflow_benchmark()
        
        if ONNX_AVAILABLE:
            self.results['onnx'] = self.onnx_benchmark()
        
        # å†…å­˜æµ‹è¯•
        self.results['memory'] = self.memory_benchmark()
        
        # ä¿å­˜ç»“æœ
        self.save_results()
        
        return self.results
    
    def save_results(self):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        # ä¿å­˜JSONæ ¼å¼ç»“æœ
        json_file = os.path.join(self.output_dir, 'benchmark_results.json')
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self.generate_markdown_report()
        
        self.log(f"æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")
    
    def generate_markdown_report(self):
        """ç”ŸæˆMarkdownæ ¼å¼çš„æµ‹è¯•æŠ¥å‘Š"""
        report_file = os.path.join(self.output_dir, 'benchmark_report.md')
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# AIåŸºå‡†æµ‹è¯•æŠ¥å‘Š\n\n")
            
            # ç³»ç»Ÿä¿¡æ¯
            if 'system_info' in self.results:
                f.write("## ç³»ç»Ÿä¿¡æ¯\n\n")
                system_info = self.results['system_info']
                f.write(f"- æµ‹è¯•æ—¶é—´: {system_info.get('timestamp', 'N/A')}\n")
                f.write(f"- Pythonç‰ˆæœ¬: {system_info.get('python_version', 'N/A')}\n")
                f.write(f"- å¹³å°: {system_info.get('platform', 'N/A')}\n")
                
                if 'gpus' in system_info:
                    f.write("- GPUä¿¡æ¯:\n")
                    for i, gpu in enumerate(system_info['gpus']):
                        f.write(f"  - GPU {i}: {gpu.get('name', 'N/A')} ({gpu.get('memory', 'N/A')})\n")
                
                if 'pytorch_version' in system_info:
                    f.write(f"- PyTorchç‰ˆæœ¬: {system_info['pytorch_version']}\n")
                
                if 'tensorflow_version' in system_info:
                    f.write(f"- TensorFlowç‰ˆæœ¬: {system_info['tensorflow_version']}\n")
                
                f.write("\n")
            
            # PyTorchç»“æœ
            if 'pytorch' in self.results:
                f.write("## PyTorchåŸºå‡†æµ‹è¯•ç»“æœ\n\n")
                self._write_pytorch_results(f, self.results['pytorch'])
            
            # TensorFlowç»“æœ
            if 'tensorflow' in self.results:
                f.write("## TensorFlowåŸºå‡†æµ‹è¯•ç»“æœ\n\n")
                self._write_tensorflow_results(f, self.results['tensorflow'])
            
            # ONNXç»“æœ
            if 'onnx' in self.results:
                f.write("## ONNX RuntimeåŸºå‡†æµ‹è¯•ç»“æœ\n\n")
                self._write_onnx_results(f, self.results['onnx'])
            
            # å†…å­˜æµ‹è¯•ç»“æœ
            if 'memory' in self.results:
                f.write("## å†…å­˜æ€§èƒ½æµ‹è¯•ç»“æœ\n\n")
                self._write_memory_results(f, self.results['memory'])
    
    def _write_pytorch_results(self, f, results):
        """å†™å…¥PyTorchæµ‹è¯•ç»“æœ"""
        if 'classification' in results:
            f.write("### å›¾åƒåˆ†ç±»æ¨¡å‹æ€§èƒ½\n\n")
            f.write("| æ¨¡å‹ | æ‰¹æ¬¡å¤§å° | å¹³å‡æ—¶é—´(ms) | ååé‡(samples/s) |\n")
            f.write("|------|----------|--------------|------------------|\n")
            
            for model_name, model_results in results['classification'].items():
                if 'error' not in model_results:
                    for batch_key, batch_results in model_results.items():
                        batch_size = batch_key.replace('batch_', '')
                        avg_time = batch_results.get('avg_time_ms', 0)
                        throughput = batch_results.get('throughput_samples_per_sec', 0)
                        f.write(f"| {model_name} | {batch_size} | {avg_time:.2f} | {throughput:.2f} |\n")
            f.write("\n")
        
        if 'compute' in results:
            f.write("### è®¡ç®—æ€§èƒ½\n\n")
            f.write("| çŸ©é˜µå¤§å° | å¹³å‡æ—¶é—´(ms) | GFLOPS |\n")
            f.write("|----------|--------------|--------|\n")
            
            for test_name, test_results in results['compute'].items():
                if 'matmul_' in test_name:
                    size = test_name.replace('matmul_', '')
                    avg_time = test_results.get('avg_time_ms', 0)
                    gflops = test_results.get('gflops', 0)
                    f.write(f"| {size} | {avg_time:.2f} | {gflops:.2f} |\n")
            f.write("\n")
    
    def _write_tensorflow_results(self, f, results):
        """å†™å…¥TensorFlowæµ‹è¯•ç»“æœ"""
        if 'classification' in results:
            f.write("### å›¾åƒåˆ†ç±»æ¨¡å‹æ€§èƒ½\n\n")
            f.write("| æ¨¡å‹ | æ‰¹æ¬¡å¤§å° | å¹³å‡æ—¶é—´(ms) | ååé‡(samples/s) |\n")
            f.write("|------|----------|--------------|------------------|\n")
            
            for model_name, model_results in results['classification'].items():
                if 'error' not in model_results:
                    for batch_key, batch_results in model_results.items():
                        batch_size = batch_key.replace('batch_', '')
                        avg_time = batch_results.get('avg_time_ms', 0)
                        throughput = batch_results.get('throughput_samples_per_sec', 0)
                        f.write(f"| {model_name} | {batch_size} | {avg_time:.2f} | {throughput:.2f} |\n")
            f.write("\n")
    
    def _write_onnx_results(self, f, results):
        """å†™å…¥ONNXæµ‹è¯•ç»“æœ"""
        if 'error' not in results:
            f.write("### ONNX Runtimeæ€§èƒ½\n\n")
            f.write("| æ‰¹æ¬¡å¤§å° | å¹³å‡æ—¶é—´(ms) | ååé‡(samples/s) |\n")
            f.write("|----------|--------------|------------------|\n")
            
            for batch_key, batch_results in results.items():
                if batch_key.startswith('batch_'):
                    batch_size = batch_key.replace('batch_', '')
                    avg_time = batch_results.get('avg_time_ms', 0)
                    throughput = batch_results.get('throughput_samples_per_sec', 0)
                    f.write(f"| {batch_size} | {avg_time:.2f} | {throughput:.2f} |\n")
            f.write("\n")
    
    def _write_memory_results(self, f, results):
        """å†™å…¥å†…å­˜æµ‹è¯•ç»“æœ"""
        for framework, framework_results in results.items():
            f.write(f"### {framework.upper()}å†…å­˜æ€§èƒ½\n\n")
            f.write("| çŸ©é˜µå¤§å° | åˆ†é…æ—¶é—´(ms) | å¤åˆ¶æ—¶é—´(ms) |\n")
            f.write("|----------|--------------|--------------|\n")
            
            for size_key, size_results in framework_results.items():
                if size_key.startswith('size_'):
                    size = size_key.replace('size_', '')
                    alloc_time = size_results.get('allocation_time_ms', 0)
                    copy_time = size_results.get('copy_time_ms', 0)
                    f.write(f"| {size} | {alloc_time:.2f} | {copy_time:.2f} |\n")
            f.write("\n")

def main():
    parser = argparse.ArgumentParser(description='AIåŸºå‡†æµ‹è¯•å·¥å…·')
    parser.add_argument('--output-dir', type=str, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--device', type=str, default='auto', 
                       choices=['auto', 'cpu', 'cuda', 'gpu'],
                       help='è®¡ç®—è®¾å¤‡')
    parser.add_argument('--framework', type=str, nargs='+', 
                       choices=['pytorch', 'tensorflow', 'onnx', 'all'],
                       default=['all'], help='è¦æµ‹è¯•çš„æ¡†æ¶')
    parser.add_argument('--test-type', type=str, nargs='+',
                       choices=['classification', 'nlp', 'compute', 'memory', 'all'],
                       default=['all'], help='è¦è¿è¡Œçš„æµ‹è¯•ç±»å‹')
    
    args = parser.parse_args()
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å®ä¾‹
    benchmark = AIBenchmark(output_dir=args.output_dir, device=args.device)
    
    # è¿è¡Œæµ‹è¯•
    if 'all' in args.framework:
        results = benchmark.run_all_benchmarks()
    else:
        results = {}
        if 'pytorch' in args.framework and TORCH_AVAILABLE:
            results['pytorch'] = benchmark.pytorch_benchmark()
        if 'tensorflow' in args.framework and TF_AVAILABLE:
            results['tensorflow'] = benchmark.tensorflow_benchmark()
        if 'onnx' in args.framework and ONNX_AVAILABLE:
            results['onnx'] = benchmark.onnx_benchmark()
        
        benchmark.results = results
        benchmark.save_results()
    
    print(f"\nğŸ‰ AIåŸºå‡†æµ‹è¯•å®Œæˆï¼")
    print(f"ç»“æœä¿å­˜åœ¨: {benchmark.output_dir}")
    print(f"è¯¦ç»†æŠ¥å‘Š: {os.path.join(benchmark.output_dir, 'benchmark_report.md')}")

if __name__ == '__main__':
    main()